{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installs and Imports\n",
    "!pip install ipympl |tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    import brax\n",
    "except:\n",
    "    !pip install git+https://github.com/google/brax.git@main |tail -n 1\n",
    "    import brax\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/instadeepai/QDax@2-instadeep-new-structure-suggestion |tail -n 1\n",
    "    import qdax\n",
    "\n",
    "\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.repertoire import compute_cvt_centroids\n",
    "from qdax import environments\n",
    "from qdax.core.neuroevolution.mdp_utils import scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffers import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.utils.plotting import plot_2d_map_elites_grid\n",
    "\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGEmitter\n",
    "from qdax.utils.metrics import CSVLogger\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'walker2d_uni'#@param['ant', 'hopper', 'walker', 'halfcheetah', 'humanoid', 'ant_omni', 'humanoid_omni']\n",
    "episode_length = 100 #@param {type:\"integer\"}\n",
    "num_iterations = 1000 #@param {type:\"integer\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "\n",
    "#@title PGA-ME Emitter Definitions Fields\n",
    "proportion_mutation_ga = 0.5\n",
    "\n",
    "# TD3 params\n",
    "env_batch_size = 100 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_size = (256, 256) #@param {type:\"raw\"}\n",
    "critic_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "greedy_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "policy_learning_rate = 1e-3 #@param {type:\"number\"}\n",
    "noise_clip = 0.5 #@param {type:\"number\"}\n",
    "policy_noise = 0.2 #@param {type:\"number\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "transitions_batch_size = 256 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "num_critic_training_steps = 300 #@param {type:\"number\"}\n",
    "num_pg_training_steps = 100 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "env = environments.create(env_name)\n",
    "\n",
    "# Init a random key\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=env_batch_size)\n",
    "fake_batch = jnp.zeros(shape=(env_batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fonction to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "  env_state,\n",
    "  policy_params,\n",
    "  random_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=env_state.info[\"state_descriptor\"],\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "def metrics_function(repertoire):\n",
    "\n",
    "    # Get metrics\n",
    "    grid_empty = repertoire.fitnesses == -jnp.inf\n",
    "    qd_score = jnp.sum(repertoire.fitnesses, where=~grid_empty)\n",
    "    # Add offset for positive qd_score\n",
    "    qd_score += reward_offset * episode_length * jnp.sum(1.0 - grid_empty)\n",
    "    coverage = 100 * jnp.mean(1.0 - grid_empty)\n",
    "    max_fitness = jnp.max(repertoire.fitnesses)\n",
    "\n",
    "    return {\n",
    "        \"qd_score\": qd_score, \"max_fitness\": max_fitness, \"coverage\": coverage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the emitter for the PG Emitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PG-emitter config\n",
    "pga_emitter_config = PGAMEConfig(\n",
    "    env_batch_size=env_batch_size,\n",
    "    batch_size=transitions_batch_size,\n",
    "    proportion_mutation_ga=proportion_mutation_ga,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    greedy_learning_rate=greedy_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=0.05, line_sigma=0.1\n",
    ")\n",
    "\n",
    "pg_emitter = PGEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial environment states\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=env_batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "init_states = reset_fn(keys)\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = environments.behavior_descriptor_extractor[env_name]\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    init_states=init_states,\n",
    "    episode_length=episode_length,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the centroids\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=pg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "repertoire, emitter_state, random_key = map_elites.init(\n",
    "    init_variables, centroids, random_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_scan_fn(carry, unused):\n",
    "    # iterate over grid\n",
    "    repertoire, emitter_state, metrics, random_key = map_elites.update(*carry)\n",
    "\n",
    "    return (repertoire, emitter_state, random_key), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = int(num_iterations / log_period)\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"pgame-logs.csv\",\n",
    "    header=[\"loop\", \"iteration\", \"qd_score\", \"max_fitness\", \"coverage\", \"time\"]\n",
    ")\n",
    "all_metrics = {}\n",
    "\n",
    "# main loop\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    # main iterations\n",
    "    (repertoire, emitter_state, random_key,), metrics = jax.lax.scan(\n",
    "        update_scan_fn,\n",
    "        (repertoire, emitter_state, random_key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # log metrics\n",
    "    logged_metrics = {\"time\": timelapse, \"loop\": 1+i, \"iteration\": 1 + i*log_period}\n",
    "    for key, value in metrics.items():\n",
    "        # take last value\n",
    "        logged_metrics[key] = value[-1]\n",
    "\n",
    "        # take all values\n",
    "        if key in all_metrics.keys():\n",
    "            all_metrics[key] = jnp.concatenate([all_metrics[key], value])\n",
    "        else:\n",
    "            all_metrics[key] = value\n",
    "\n",
    "    csv_logger.log(logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# Customize matplotlib params\n",
    "font_size = 16\n",
    "params = {\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"text.usetex\": False,\n",
    "    \"axes.titlepad\": 10,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "# Visualize the training evolution and final repertoire\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(40,10))\n",
    "\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * batch_size\n",
    "\n",
    "axes[0].plot(env_steps, all_metrics['coverage'])\n",
    "axes[0].set_xlabel('Environment steps')\n",
    "axes[0].set_ylabel('Coverage in %')\n",
    "axes[0].set_title('Coverage evolution during training')\n",
    "axes[0].set_aspect(0.95/axes[0].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[1].plot(env_steps, all_metrics['max_fitness'])\n",
    "axes[1].set_xlabel('Environment steps')\n",
    "axes[1].set_ylabel('Maximum fitness')\n",
    "axes[1].set_title('Maximum fitness evolution during training')\n",
    "axes[1].set_aspect(0.95/axes[1].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[2].plot(env_steps, all_metrics['qd_score'])\n",
    "axes[2].set_xlabel('Environment steps')\n",
    "axes[2].set_ylabel('QD Score')\n",
    "axes[2].set_title('QD Score evolution during training')\n",
    "axes[2].set_aspect(0.95/axes[2].get_data_ratio(), adjustable='box')\n",
    "\n",
    "plot_2d_map_elites_grid(\n",
    "    centroids=centroids,\n",
    "    grid_fitness=repertoire.fitnesses,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    "    grid_descriptors=repertoire.descriptors,\n",
    "    ax=axes[3],\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
