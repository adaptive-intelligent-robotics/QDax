{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installs and Imports\n",
    "!pip install ipympl |tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    import brax\n",
    "except:\n",
    "    !pip install git+https://github.com/google/brax.git@main |tail -n 1\n",
    "    import brax\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/instadeepai/QDax@2-instadeep-new-structure-suggestion |tail -n 1\n",
    "    import qdax\n",
    "\n",
    "\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax import environments\n",
    "from qdax.core.neuroevolution.mdp_utils import scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffers import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.plotting import plot_2d_map_elites_grid\n",
    "\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGEmitter\n",
    "from qdax.utils.metrics import CSVLogger\n",
    "\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "batch_size = 100 #@param {type:\"number\"}\n",
    "env_name = 'walker2d_uni'#@param['ant', 'hopper', 'walker', 'halfcheetah', 'humanoid', 'ant_omni', 'humanoid_omni']\n",
    "episode_length = 100 #@param {type:\"integer\"}\n",
    "num_iterations = 1000 #@param {type:\"integer\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brax.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "env = environments.create(env_name)\n",
    "\n",
    "# Init a random key\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    kernel_init_final=jax.nn.initializers.uniform(1e-3),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=batch_size)\n",
    "fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "\n",
    "\n",
    "# Create the initial environment states\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "init_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fonction to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "  env_state,\n",
    "  policy_params,\n",
    "  random_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=env_state.info[\"state_descriptor\"],\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scoring function, the emitter used and the way metrics are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "bd_extraction_fn = environments.behavior_descriptor_extractor[env_name]\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    init_states=init_states,\n",
    "    episode_length=episode_length,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "\n",
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=0.05, line_sigma=0.1\n",
    ")\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=None, \n",
    "    variation_fn=variation_fn, \n",
    "    variation_percentage=1.0, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "def metrics_function(repertoire):\n",
    "\n",
    "    # Get metrics\n",
    "    grid_empty = repertoire.fitnesses == -jnp.inf\n",
    "    qd_score = jnp.sum(repertoire.fitnesses, where=~grid_empty)\n",
    "    # Add offset for positive qd_score\n",
    "    qd_score += reward_offset * episode_length * jnp.sum(1.0 - grid_empty)\n",
    "    coverage = 100 * jnp.mean(1.0 - grid_empty)\n",
    "    max_fitness = jnp.max(repertoire.fitnesses)\n",
    "\n",
    "    return {\n",
    "        \"qd_score\": qd_score, \"max_fitness\": max_fitness, \"coverage\": coverage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and initialise the MAP Elites algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MAP-Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=mixing_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    ")\n",
    "\n",
    "# Compute initial repertoire\n",
    "repertoire, _, random_key = map_elites.init(init_variables, centroids, random_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare scan over map_elites update to perform several iterations at a time\n",
    "@jax.jit\n",
    "def update_scan_fn(carry, unused):\n",
    "    # iterate over grid\n",
    "    repertoire, random_key = carry\n",
    "    (repertoire, _, metrics, random_key,) = map_elites.update(\n",
    "        repertoire,\n",
    "        None,\n",
    "        random_key,\n",
    "    )\n",
    "\n",
    "    return (repertoire, random_key), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch MAP-Elites iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = int(num_iterations / log_period)\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs.csv\",\n",
    "    header=[\"loop\", \"iteration\", \"qd_score\", \"max_fitness\", \"coverage\", \"time\"]\n",
    ")\n",
    "all_metrics = {}\n",
    "\n",
    "# main loop\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    # main iterations\n",
    "    (repertoire, random_key,), metrics = jax.lax.scan(\n",
    "        update_scan_fn,\n",
    "        (repertoire, random_key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # log metrics\n",
    "    logged_metrics = {\"time\": timelapse, \"loop\": 1+i, \"iteration\": 1 + i*log_period}\n",
    "    for key, value in metrics.items():\n",
    "        # take last value\n",
    "        logged_metrics[key] = value[-1]\n",
    "\n",
    "        # take all values\n",
    "        if key in all_metrics.keys():\n",
    "            all_metrics[key] = jnp.concatenate([all_metrics[key], value])\n",
    "        else:\n",
    "            all_metrics[key] = value\n",
    "\n",
    "    csv_logger.log(logged_metrics)\n",
    "    print(logged_metrics)\n",
    "\n",
    "# close the file\n",
    "csv_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# Customize matplotlib params\n",
    "font_size = 16\n",
    "params = {\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"text.usetex\": False,\n",
    "    \"axes.titlepad\": 10,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "# Visualize the training evolution and final repertoire\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(40,10))\n",
    "\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * batch_size\n",
    "\n",
    "axes[0].plot(env_steps, all_metrics['coverage'])\n",
    "axes[0].set_xlabel('Environment steps')\n",
    "axes[0].set_ylabel('Coverage in %')\n",
    "axes[0].set_title('Coverage evolution during training')\n",
    "axes[0].set_aspect(0.95/axes[0].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[1].plot(env_steps, all_metrics['max_fitness'])\n",
    "axes[1].set_xlabel('Environment steps')\n",
    "axes[1].set_ylabel('Maximum fitness')\n",
    "axes[1].set_title('Maximum fitness evolution during training')\n",
    "axes[1].set_aspect(0.95/axes[1].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[2].plot(env_steps, all_metrics['qd_score'])\n",
    "axes[2].set_xlabel('Environment steps')\n",
    "axes[2].set_ylabel('QD Score')\n",
    "axes[2].set_title('QD Score evolution during training')\n",
    "axes[2].set_aspect(0.95/axes[2].get_data_ratio(), adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_2d_map_elites_grid(\n",
    "    centroids=centroids,\n",
    "    grid_fitness=repertoire.fitnesses,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    "    grid_descriptors=repertoire.descriptors,\n",
    "    ax=None,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store metrics and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the emitter for the PG Emitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title PGA-ME Emitter Definitions Fields\n",
    "#@markdown ---\n",
    "proportion_mutation_ga = 0.5\n",
    "\n",
    "# TD3 params\n",
    "env_batch_size = 100 #@param {type:\"number\"}\n",
    "batch_size = 256 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_size = (256, 256) #@param {type:\"raw\"}\n",
    "critic_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "greedy_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "policy_learning_rate = 1e-3 #@param {type:\"number\"}\n",
    "noise_clip = 0.5 #@param {type:\"number\"}\n",
    "policy_noise = 0.2 #@param {type:\"number\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "transitions_batch_size = 256 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "num_critic_training_steps = 300 #@param {type:\"number\"}\n",
    "num_pg_training_steps = 100 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=env_batch_size)\n",
    "fake_batch = jnp.zeros(shape=(env_batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PG-emitter config\n",
    "pga_emitter_config = PGAMEConfig(\n",
    "    env_batch_size=env_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    proportion_mutation_ga=proportion_mutation_ga,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    greedy_learning_rate=greedy_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the emitter\n",
    "crossover_fn = functools.partial(\n",
    "    isoline_crossover_function, iso_sigma=0.05, line_sigma=0.1\n",
    ")\n",
    "\n",
    "pg_emitter = PGEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    crossover_fn=crossover_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial environment states\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=env_batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "init_states = reset_fn(keys)\n",
    "\n",
    "# Prepare the scoring function\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "bd_extraction_fn = brax_envs.behavior_descriptor_extractor[env_name]\n",
    "scoring_function = functools.partial(\n",
    "    scoring_function,\n",
    "    init_states=init_states,\n",
    "    episode_length=episode_length,\n",
    "    random_key=random_key,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the centroids\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_function,\n",
    "    emitter=pg_emitter,\n",
    "    metrics_function=metrics_fn,\n",
    ")\n",
    "\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "repertoire, emitter_state = map_elites.init_fn(\n",
    "    init_variables, centroids, subkey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_scan_fn(carry, unused):\n",
    "    # iterate over grid\n",
    "    repertoire, emitter_state, metrics, random_key = map_elites.update_fn(*carry)\n",
    "\n",
    "    return (repertoire, emitter_state, random_key), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the algorithm\n",
    "(repertoire, emitter_state, random_key,), metrics = jax.lax.scan(\n",
    "    update_scan_fn,\n",
    "    (repertoire, emitter_state, random_key),\n",
    "    (),\n",
    "    length=num_iterations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# Customize matplotlib params\n",
    "font_size = 16\n",
    "params = {\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"text.usetex\": False,\n",
    "    \"axes.titlepad\": 10,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "# Visualize the training evolution and final repertoire\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(40,10))\n",
    "\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * batch_size\n",
    "\n",
    "axes[0].plot(env_steps, metrics['coverage'])\n",
    "axes[0].set_xlabel('Environment steps')\n",
    "axes[0].set_ylabel('Coverage in %')\n",
    "axes[0].set_title('Coverage evolution during training')\n",
    "axes[0].set_aspect(0.95/axes[0].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[1].plot(env_steps, metrics['max_fitness'])\n",
    "axes[1].set_xlabel('Environment steps')\n",
    "axes[1].set_ylabel('Maximum fitness')\n",
    "axes[1].set_title('Maximum fitness evolution during training')\n",
    "axes[1].set_aspect(0.95/axes[1].get_data_ratio(), adjustable='box')\n",
    "\n",
    "axes[2].plot(env_steps, metrics['qd_score'])\n",
    "axes[2].set_xlabel('Environment steps')\n",
    "axes[2].set_ylabel('QD Score')\n",
    "axes[2].set_title('QD Score evolution during training')\n",
    "axes[2].set_aspect(0.95/axes[2].get_data_ratio(), adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_2d_map_elites_grid(\n",
    "    centroids=centroids,\n",
    "    grid_fitness=repertoire.fitnesses,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    "    grid_descriptors=repertoire.descriptors,\n",
    "    ax=None,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to save/load a repertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire_path = \"./last_repertoire/\"\n",
    "os.makedirs(repertoire_path, exist_ok=True)\n",
    "repertoire.save(path=repertoire_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the reconstruction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population of policies\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "fake_batch = jnp.zeros(shape=(env.observation_size,))\n",
    "fake_params = policy_network.init(subkey, fake_batch)\n",
    "\n",
    "_, reconstruction_fn = ravel_pytree(fake_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire = MapElitesRepertoire.load(reconstruction_fn=reconstruction_fn, path=repertoire_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = jnp.argmax(repertoire.fitnesses)\n",
    "best_fitness = jnp.max(repertoire.fitnesses)\n",
    "best_bd = repertoire.descriptors[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fitness, best_bd, best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = jax.tree_util.tree_map(\n",
    "    lambda x: x[best_idx],\n",
    "    repertoire.genotypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_env_reset = jax.jit(env.reset)\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(policy_network.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = []\n",
    "rng = jax.random.PRNGKey(seed=1)\n",
    "state = jit_env_reset(rng=rng)\n",
    "while not state.done:\n",
    "    rollout.append(state)\n",
    "    action = jit_inference_fn(my_params, state.obs)\n",
    "    state = jit_env_step(state, action)\n",
    "\n",
    "print(len(rollout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(html.render(env.sys, [s.qp for s in rollout[:500]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
