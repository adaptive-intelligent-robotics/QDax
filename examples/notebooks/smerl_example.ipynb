{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adaptive-intelligent-robotics/QDax/blob/main/notebooks/smerl_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DIAYN SMERL with Jax\n",
    "\n",
    "This notebook shows how to use QDax to train DIAYN SMERL on a Brax environment. It can be run locally or on Google Colab. We recommand to use a GPU. This notebook will show:\n",
    "- how to define an environment\n",
    "- how to define a replay buffer\n",
    "- how to create a diayn smerl instance\n",
    "- which functions must be defined before training\n",
    "- how to launch a certain number of training steps\n",
    "- how to visualise the final trajectories learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installs and Imports\n",
    "!pip install ipympl |tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    import brax\n",
    "except:\n",
    "    !pip install git+https://github.com/google/brax.git@v0.0.15 |tail -n 1\n",
    "    import \n",
    "    \n",
    "try:\n",
    "    import haiku\n",
    "except:\n",
    "    !pip install git+https://github.com/deepmind/dm-haiku.git@v0.0.5 |tail -n 1\n",
    "    import haiku\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/adaptive-intelligent-robotics/QDax@main |tail -n 1\n",
    "    import qdax\n",
    "\n",
    "\n",
    "from qdax import environments\n",
    "from qdax.baselines.diayn_smerl import DIAYNSMERL, DiaynSmerlConfig, DiaynTrainingState\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.buffers.trajectory_buffer import TrajectoryBuffer\n",
    "from qdax.core.neuroevolution.sac_utils import do_iteration_fn, warmstart_buffer\n",
    "\n",
    "from qdax.utils.plotting import plot_skills_trajectory\n",
    "\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters choice\n",
    "\n",
    "Most hyperparameters are similar to those introduced in [SAC paper](https://arxiv.org/abs/1801.01290), [DIAYN paper](https://arxiv.org/abs/1802.06070) and [SMERL paper](https://arxiv.org/pdf/2010.14484.pdf).\n",
    "\n",
    "The parameter `descriptor_full_state` is less straightforward, it concerns the information used for diversity seeking and discrimination. In DIAYN, one can use the full state for diversity seeking, but one can also use a prior to focus on an interesting aspect of the state. Actually, priors are often used in experiments, for instance, focusing on the x/y position rather than the full position. When `descriptor_full_state` is set to True, it uses the full state, when it is set to False, it uses the 'state descriptor' retrieved by the environment. Hence, it is required that the environment has one. (All the `_uni`, `_omni` do, same for `anttrap`, `antmaze` and `pointmaze`.) In the future, we will add an option to use a prior function direclty on the full state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'anttrap' #@param['ant_uni', 'hopper_uni', 'walker2d_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "seed = 0 #@param {type:\"integer\"}\n",
    "env_batch_size = 250 #@param {type:\"integer\"}\n",
    "num_steps = 1000000 #@param {type:\"integer\"}\n",
    "warmup_steps = 0 #@param {type:\"integer\"}\n",
    "buffer_size = 1000000 #@param {type:\"integer\"}\n",
    "\n",
    "# SAC config\n",
    "batch_size = 256 #@param {type:\"integer\"}\n",
    "episode_length = 100 #@param {type:\"integer\"}\n",
    "grad_updates_per_step = 0.25 #@param {type:\"number\"}\n",
    "tau = 0.005 #@param {type:\"number\"}\n",
    "learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "alpha_init = 1.0 #@param {type:\"number\"}\n",
    "discount = 0.97 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "fix_alpha = False #@param {type:\"boolean\"}\n",
    "normalize_observations = False #@param {type:\"boolean\"}\n",
    "# DIAYN config\n",
    "num_skills = 5 #@param {type:\"integer\"}\n",
    "descriptor_full_state = False #@param {type:\"boolean\"}\n",
    "# SMERL specific\n",
    "diversity_reward_scale = 2.0 #@param {type:\"number\"}\n",
    "smerl_target = 800 #@param {type:\"number\"}\n",
    "smerl_margin = 800 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment and replay buffer\n",
    "\n",
    "Define the environment in which the policies will be trained. In this notebook, we focus on controllers learning to move a robot in a physical simulation.\n",
    "\n",
    "For DIAYN SMERL, we need to use a called Trajectory Replay Buffer, because the reward depend on the trajectory accumulated rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environments\n",
    "assert (\n",
    "    env_batch_size % num_skills == 0\n",
    "), \"Parameter env_batch_size should be a multiple of num_skills\"\n",
    "num_env_per_skill = env_batch_size // num_skills\n",
    "\n",
    "env = environments.create(\n",
    "    env_name=env_name,\n",
    "    batch_size=env_batch_size,\n",
    "    episode_length=episode_length,\n",
    "    auto_reset=True,\n",
    ")\n",
    "\n",
    "eval_env = environments.create(\n",
    "    env_name=env_name,\n",
    "    batch_size=env_batch_size,\n",
    "    episode_length=episode_length,\n",
    "    auto_reset=True,\n",
    "    eval_metrics=True,\n",
    ")\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "env_state = jax.jit(env.reset)(rng=key)\n",
    "eval_env_first_state = jax.jit(eval_env.reset)(rng=key)\n",
    "\n",
    "# Initialize buffer\n",
    "dummy_transition = QDTransition.init_dummy(\n",
    "    observation_dim=env.observation_size + num_skills,\n",
    "    action_dim=env.action_size,\n",
    "    descriptor_dim=env.behavior_descriptor_length,\n",
    ")\n",
    "\n",
    "# Use a trajectory replay buffer\n",
    "replay_buffer = TrajectoryBuffer.init(\n",
    "    buffer_size=buffer_size,\n",
    "    transition=dummy_transition,\n",
    "    env_batch_size=env_batch_size,\n",
    "    episode_length=episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the config, instantiate and initialize DIAYN SMERL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diayn_smerl_config = DiaynSmerlConfig(\n",
    "    # SAC config\n",
    "    batch_size=batch_size,\n",
    "    episode_length=episode_length,\n",
    "    grad_updates_per_step=grad_updates_per_step,\n",
    "    tau=tau,\n",
    "    normalize_observations=normalize_observations,\n",
    "    learning_rate=learning_rate,\n",
    "    alpha_init=alpha_init,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    fix_alpha=fix_alpha,\n",
    "    # DIAYN config\n",
    "    num_skills=num_skills,\n",
    "    descriptor_full_state=descriptor_full_state,\n",
    "    diversity_reward_scale=diversity_reward_scale,\n",
    "    smerl_margin=smerl_margin,\n",
    "    smerl_target=smerl_target,\n",
    ")\n",
    "\n",
    "# define an instance of DIAYN SMERL\n",
    "diayn_smerl = DIAYNSMERL(config=diayn_smerl_config, action_size=env.action_size)\n",
    "\n",
    "if descriptor_full_state:\n",
    "    descriptor_size = env.observation_size\n",
    "else:\n",
    "    descriptor_size = env.behavior_descriptor_length\n",
    "\n",
    "# get the initial training state\n",
    "training_state = diayn_smerl.init(\n",
    "    key,\n",
    "    action_size=env.action_size,\n",
    "    observation_size=env.observation_size,\n",
    "    descriptor_size=descriptor_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the skills and the policy evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replications of the same skill are evaluated in parallel\n",
    "skills = jnp.concatenate(\n",
    "    [jnp.eye(num_skills)] * num_env_per_skill,\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Make play_step functions scannable by passing static args beforehand\n",
    "play_eval_step = functools.partial(\n",
    "    diayn_smerl.play_step_fn,\n",
    "    skills=skills,\n",
    "    env=eval_env,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "play_step = functools.partial(\n",
    "    diayn_smerl.play_step_fn,\n",
    "    skills=skills,\n",
    "    env=env,\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "eval_policy = functools.partial(\n",
    "    diayn_smerl.eval_policy_fn,\n",
    "    play_step_fn=play_eval_step,\n",
    "    eval_env_first_state=eval_env_first_state,\n",
    "    env_batch_size=env_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmstart the buffer\n",
    "\n",
    "One can fill the replay buffer before the beginning of the training to reduce instabilities in the first steps of the training. This step is not required at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmstart the buffer\n",
    "replay_buffer, env_state, training_state = warmstart_buffer(\n",
    "    replay_buffer=replay_buffer,\n",
    "    training_state=training_state,\n",
    "    env_state=env_state,\n",
    "    num_warmstart_steps=warmup_steps,\n",
    "    env_batch_size=env_batch_size,\n",
    "    play_step_fn=play_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare last utils for the training loop\n",
    "\n",
    "Many Reinforcement Learning algorithm have similar training process, that can be divided in a precise training step that is repeated several times. Most of the differences are captured inside the `play_step` and in the `update` functions. Hence, once those are defined, the iteration works in the same way. For this reason, instead of coding the same function for each algorithm, we have created the `do_iteration_fn` that can be used by most of them. In the training script, the user just has to partial the function to give `play_step`, `update` plus a few other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from brax.envs import State as EnvState\n",
    "\n",
    "total_num_iterations = num_steps // env_batch_size\n",
    "\n",
    "# fix static arguments - prepare for scan\n",
    "do_iteration = functools.partial(\n",
    "    do_iteration_fn,\n",
    "    env_batch_size=env_batch_size,\n",
    "    grad_updates_per_step=grad_updates_per_step,\n",
    "    play_step_fn=play_step,\n",
    "    update_fn=diayn_smerl.update,\n",
    ")\n",
    "\n",
    "# define a function that enables do_iteration to be scanned\n",
    "@jax.jit\n",
    "def _scan_do_iteration(\n",
    "    carry: Tuple[DiaynTrainingState, EnvState, TrajectoryBuffer],\n",
    "    unused_arg: Any,\n",
    ") -> Tuple[Tuple[DiaynTrainingState, EnvState, TrajectoryBuffer], Any]:\n",
    "    (\n",
    "        training_state,\n",
    "        env_state,\n",
    "        replay_buffer,\n",
    "        metrics,\n",
    "    ) = do_iteration(*carry)\n",
    "    return (training_state, env_state, replay_buffer), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Training loop: this is a scan of the `do_iteration_fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Main loop\n",
    "(training_state, env_state, replay_buffer), metrics = jax.lax.scan(\n",
    "    _scan_do_iteration,\n",
    "    (training_state, env_state, replay_buffer),\n",
    "    (),\n",
    "    length=total_num_iterations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the trajectories of the skills at the end of the training\n",
    "\n",
    "This only works when the state descriptor considered is two-dimensional, and as a real interest only when this state descriptor is the x/y position. Hence, on all \"omni\" tasks, on pointmaze, anttrap and antmaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation part\n",
    "true_return, true_returns, diversity_returns, state_desc = eval_policy(\n",
    "    training_state=training_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trajectory of the skills\n",
    "fig, ax = plot_skills_trajectory(\n",
    "    trajectories=state_desc.T,\n",
    "    skills=skills,\n",
    "    min_values=[0, -8],\n",
    "    max_values=[30, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the skills in the physical simulation\n",
    "\n",
    "WARNING: this does not work with \"pointmaze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env_name != \"pointmaze\", \"No visualisation available for pointmaze at the moment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_skill = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = training_state.policy_params\n",
    "\n",
    "possible_skills = jnp.eye(num_skills)\n",
    "skill = possible_skills[my_skill]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment and jit the step and inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an environment that is not vectorized\n",
    "visual_env = environments.create(\n",
    "    env_name=env_name,\n",
    "    episode_length=episode_length,\n",
    "    auto_reset=True,\n",
    ")\n",
    "\n",
    "# jit reset/step/inference functions\n",
    "jit_env_reset = jax.jit(visual_env.reset)\n",
    "jit_env_step = jax.jit(visual_env.step)\n",
    "\n",
    "@jax.jit\n",
    "def jit_inference_fn(params, observation, random_key):\n",
    "    obs = jnp.concatenate([observation, skill], axis=0)\n",
    "    action, random_key = diayn_smerl.select_action(obs, params, random_key, deterministic=True)\n",
    "    return action, random_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout in the environment and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = []\n",
    "random_key = jax.random.PRNGKey(seed=1)\n",
    "state = jit_env_reset(rng=random_key)\n",
    "while not state.done:\n",
    "    rollout.append(state)\n",
    "    action, random_key = jit_inference_fn(my_params, state.obs, random_key)\n",
    "    state = jit_env_step(state, action)\n",
    "\n",
    "print(f\"The trajectory of this individual contains {len(rollout)} transitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(html.render(visual_env.sys, [s.qp for s in rollout[:500]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
