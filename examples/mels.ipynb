{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adaptive-intelligent-robotics/QDax/blob/main/examples/mels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Uncertain Domains with ME-LS in JAX\n",
    "\n",
    "This notebook shows how to discover controllers that achieve consistent performance in MDP domains using the [MAP-Elites Low-Spread](https://dl.acm.org/doi/abs/10.1145/3583131.3590433) algorithm. It can be run locally or on Google Colab. We recommend to use a GPU. This notebook will show:\n",
    "\n",
    "- how to define the problem\n",
    "- how to create an emitter\n",
    "- how to create an ME-LS instance\n",
    "- which functions must be defined before training\n",
    "- how to launch a certain number of training steps\n",
    "- how to visualise the optimization process\n",
    "- how to save/load a repertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    print(\"QDax not found. Installing...\")\n",
    "    !pip install qdax[cuda12]\n",
    "    import qdax\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install ipympl |tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.mels import MELS\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids\n",
    "from qdax.core.containers.mels_repertoire import MELSRepertoire\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "from IPython.display import HTML\n",
    "from brax.v1.io import html\n",
    "\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "batch_size = 100 #@param {type:\"number\"}\n",
    "env_name = 'walker2d_uni'#@param['ant_uni', 'hopper_uni', 'walker2d_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "num_samples = 5 #@param {type:\"number\"}\n",
    "episode_length = 100 #@param {type:\"integer\"}\n",
    "num_iterations = 1000 #@param {type:\"integer\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "iso_sigma = 0.005 #@param {type:\"number\"}\n",
    "line_sigma = 0.05 #@param {type:\"number\"}\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env\n",
    "\n",
    "Define the environment in which the policies will be trained. In this notebook, we consider the problem where each controller is evaluated `num_samples` times, each time in a different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init environment\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "\n",
    "# Init a random key\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers. There are batch_size controllers, and each\n",
    "# controller will be evaluated num_samples times.\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=batch_size)\n",
    "fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env\n",
    "\n",
    "Now that the environment and policy has been defined, it is necessary to define a function that describes how the policy must be used to interact with the environment and to store transition data. This is identical to the function in the MAP-Elites tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "    env_state,\n",
    "    policy_params,\n",
    "    random_key,\n",
    "):\n",
    "    \"\"\"Play an environment step and return the updated state and the\n",
    "    transition.\"\"\"\n",
    "\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scoring function and the way metrics are computed\n",
    "\n",
    "The scoring function is used in the evaluation step to determine the fitness and behavior descriptor of each individual. Note that while the MAP-Elites tutorial uses `scoring_function_brax_envs` as the basis for the scoring function, we use `reset_based_scoring_function_brax_envs`. The difference is that `reset_based_scoring_function_brax_envs` generates initial states randomly instead of taking in a fixed set of initial states. This is necessary since we are evaluating each controller across sampled initial states. If the initial states were kept the same for all evaluations, there would be no stochasticity in the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "bd_extraction_fn = environments.behavior_descriptor_extractor[env_name]\n",
    "scoring_fn = functools.partial(\n",
    "    reset_based_scoring_function_brax_envs,\n",
    "    episode_length=episode_length,\n",
    "    play_reset_fn=env.reset,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_fn = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the emitter\n",
    "\n",
    "The emitter is used to evolve the population at each mutation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=None,\n",
    "    variation_fn=variation_fn,\n",
    "    variation_percentage=1.0,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and initialise the ME-LS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate ME-LS.\n",
    "mels = MELS(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=mixing_emitter,\n",
    "    metrics_function=metrics_fn,\n",
    "    num_samples=num_samples,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    "    random_key=random_key,\n",
    ")\n",
    "\n",
    "# Compute initial repertoire and emitter state\n",
    "repertoire, emitter_state, random_key = mels.init(init_variables, centroids, random_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch ME-LS iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = int(num_iterations / log_period)\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"mapelites-logs.csv\",\n",
    "    header=[\"loop\", \"iteration\", \"qd_score\", \"max_fitness\", \"coverage\", \"time\"]\n",
    ")\n",
    "all_metrics = {}\n",
    "\n",
    "# main loop\n",
    "mels_scan_update = mels.scan_update\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    # main iterations\n",
    "    (repertoire, emitter_state, random_key,), metrics = jax.lax.scan(\n",
    "        mels_scan_update,\n",
    "        (repertoire, emitter_state, random_key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # log metrics\n",
    "    logged_metrics = {\"time\": timelapse, \"loop\": 1+i, \"iteration\": 1 + i*log_period}\n",
    "    for key, value in metrics.items():\n",
    "        # take last value\n",
    "        logged_metrics[key] = value[-1]\n",
    "\n",
    "        # take all values\n",
    "        if key in all_metrics.keys():\n",
    "            all_metrics[key] = jnp.concatenate([all_metrics[key], value])\n",
    "        else:\n",
    "            all_metrics[key] = value\n",
    "\n",
    "    csv_logger.log(logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# create the x-axis array\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * batch_size\n",
    "\n",
    "# create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(env_steps=env_steps, metrics=all_metrics, repertoire=repertoire, min_bd=min_bd, max_bd=max_bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to save/load a repertoire\n",
    "\n",
    "The following cells show how to save or load a repertoire of individuals and add a few lines to visualise the best performing individual in a simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the final repertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire_path = \"./last_repertoire/\"\n",
    "os.makedirs(repertoire_path, exist_ok=True)\n",
    "repertoire.save(path=repertoire_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the reconstruction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population of policies\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "fake_batch = jnp.zeros(shape=(env.observation_size,))\n",
    "fake_params = policy_network.init(subkey, fake_batch)\n",
    "\n",
    "_, reconstruction_fn = ravel_pytree(fake_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the reconstruction function to load and re-create the repertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire = MELSRepertoire.load(reconstruction_fn=reconstruction_fn, path=repertoire_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best individual of the repertoire\n",
    "\n",
    "Note that in ME-LS, the individual's cell is computed by finding its most frequent archive cell among its `num_samples` behavior descriptors. Thus, the descriptor associated with each individual in the archive is not its mean descriptor. Rather, we set the descriptor in the archive to be the centroid of the individual's most frequent archive cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = jnp.argmax(repertoire.fitnesses)\n",
    "best_fitness = jnp.max(repertoire.fitnesses)\n",
    "best_bd = repertoire.descriptors[best_idx]\n",
    "best_spread = repertoire.spreads[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Best fitness in the repertoire: {best_fitness:.2f}\\n\"\n",
    "    f\"Behavior descriptor of the best individual in the repertoire: {best_bd}\\n\"\n",
    "    f\"Spread of the best individual in the repertoire: {best_spread}\\n\"\n",
    "    f\"Index in the repertoire of this individual: {best_idx}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = jax.tree_util.tree_map(\n",
    "    lambda x: x[best_idx],\n",
    "    repertoire.genotypes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play some steps in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_env_reset = jax.jit(env.reset)\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(policy_network.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = []\n",
    "rng = jax.random.PRNGKey(seed=1)\n",
    "state = jit_env_reset(rng=rng)\n",
    "while not state.done:\n",
    "    rollout.append(state)\n",
    "    action = jit_inference_fn(my_params, state.obs)\n",
    "    state = jit_env_step(state, action)\n",
    "\n",
    "print(f\"The trajectory of this individual contains {len(rollout)} transitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HTML(html.render(env.sys, [s.qp for s in rollout[:500]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
