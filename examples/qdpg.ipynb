{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adaptive-intelligent-robotics/QDax/blob/main/examples/qdpg.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with QDPG in Jax\n",
    "\n",
    "This notebook shows how to use QDax to find diverse and performing controllers in MDPs with [QDPG - Quality Diversity Policy Gradient in MAP-Elites](https://arxiv.org/abs/2006.08505).\n",
    "It can be run locally or on Google Colab. We recommand to use a GPU. This notebook will show:\n",
    "\n",
    "- how to define the problem\n",
    "- how to create the QDPG emitter\n",
    "- how to create a Map-elites instance\n",
    "- which functions must be defined before training\n",
    "- how to launch a certain number of training steps\n",
    "- how to visualize the results of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installs and Imports\n",
    "!pip install ipympl |tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    import brax\n",
    "except:\n",
    "    !pip install git+https://github.com/google/brax.git@v0.10.4 |tail -n 1\n",
    "    import brax\n",
    "\n",
    "try:\n",
    "    import flax\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/google/flax.git@v0.8.5 |tail -n 1\n",
    "    import flax\n",
    "\n",
    "try:\n",
    "    import chex\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/deepmind/chex.git@v0.1.86 |tail -n 1\n",
    "    import chex\n",
    "\n",
    "try:\n",
    "    import jumanji\n",
    "except:\n",
    "    !pip install \"jumanji==0.3.1\"\n",
    "    import jumanji\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/adaptive-intelligent-robotics/QDax@main |tail -n 1\n",
    "    import qdax\n",
    "\n",
    "from qdax.core.containers.archive import score_euclidean_novelty\n",
    "from qdax.core.emitters.dpg_emitter import DiversityPGConfig\n",
    "from qdax.core.emitters.qdpg_emitter import QDPGEmitter, QDPGEmitterConfig\n",
    "from qdax.core.emitters.qpg_emitter import QualityPGConfig\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGAMEEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'walker2d_uni'#@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "episode_length = 250 #@param {type:\"integer\"}\n",
    "num_iterations = 100 #@param {type:\"integer\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "iso_sigma = 0.005 #@param {type:\"number\"}\n",
    "line_sigma = 0.05 #@param {type:\"number\"}\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "\n",
    "# mutations size\n",
    "quality_pg_batch_size = 34 #@param {type:\"integer\"}\n",
    "diversity_pg_batch_size = 33 #@param {type:\"integer\"}\n",
    "ga_batch_size = 33 #@param {type:\"integer\"}\n",
    "\n",
    "env_batch_size = quality_pg_batch_size + diversity_pg_batch_size + ga_batch_size\n",
    "\n",
    "# TD3 params\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_size = (256, 256) #@param {type:\"raw\"}\n",
    "critic_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "greedy_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "policy_learning_rate = 1e-3 #@param {type:\"number\"}\n",
    "noise_clip = 0.5 #@param {type:\"number\"}\n",
    "policy_noise = 0.2 #@param {type:\"number\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "transitions_batch_size = 256 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "num_critic_training_steps = 300 #@param {type:\"number\"}\n",
    "num_pg_training_steps = 100 #@param {type:\"number\"}\n",
    "policy_delay = 2 #@param {type:\"number\"}\n",
    "\n",
    "archive_acceptance_threshold = 0.1 #@param {type:\"number\"}\n",
    "archive_max_size = 10000 #@param {type:\"integer\"}\n",
    "\n",
    "num_nearest_neighb = 5 #@param {type:\"integer\"}\n",
    "novelty_scaling_ratio = 1.0 #@param {type:\"number\"}\n",
    "#@markdown --- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env\n",
    "\n",
    "Define the environment in which the policies will be trained. In this notebook, we focus on controllers learning to move a robot in a physical simulation. We also define the shared policy, that every individual in the population will use. Once the policy is defined, all individuals are defined by their parameters, that corresponds to their genotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "\n",
    "# Init a random key\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=env_batch_size)\n",
    "fake_batch = jnp.zeros(shape=(env_batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "\n",
    "# Create the initial environment states\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=env_batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "init_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fonction to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "  env_state,\n",
    "  policy_params,\n",
    "  random_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scoring function and the way metrics are computed\n",
    "\n",
    "The scoring function is used in the evaluation step to determine the fitness and behavior descriptor of each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "bd_extraction_fn = environments.behavior_descriptor_extractor[env_name]\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    init_states=init_states,\n",
    "    episode_length=episode_length,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the emitter: QDPG Emitter\n",
    "\n",
    "The emitter is used to evolve the population at each mutation step. In this example, the emitter is the Quality Diversity Policy Gradient emitter, the one used in QDPG. It trains two critics with the transitions experienced in the environment. One with extrinsic reward from the environment, the other uses diversity rewards based on the novelty of the states encountered. The critic are then used to apply Policy Gradient updates to the policies evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Quality PG emitter config\n",
    "qpg_emitter_config = QualityPGConfig(\n",
    "    env_batch_size=quality_pg_batch_size,\n",
    "    batch_size=transitions_batch_size,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    actor_learning_rate=greedy_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps,\n",
    "    policy_delay=policy_delay,\n",
    ")\n",
    "\n",
    "# Define the Diversity PG emitter config\n",
    "dpg_emitter_config = DiversityPGConfig(\n",
    "    env_batch_size=diversity_pg_batch_size,\n",
    "    batch_size=transitions_batch_size,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    actor_learning_rate=greedy_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps,\n",
    "    policy_delay=policy_delay,\n",
    "    archive_acceptance_threshold=archive_acceptance_threshold,\n",
    "    archive_max_size=archive_max_size,\n",
    ")\n",
    "\n",
    "# Define the QDPG Emitter config\n",
    "qdpg_emitter_config = QDPGEmitterConfig(\n",
    "    qpg_config=qpg_emitter_config,\n",
    "    dpg_config=dpg_emitter_config,\n",
    "    iso_sigma=iso_sigma,\n",
    "    line_sigma=line_sigma,\n",
    "    ga_batch_size=ga_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the emitter\n",
    "score_novelty = jax.jit(\n",
    "    functools.partial(\n",
    "        score_euclidean_novelty,\n",
    "        num_nearest_neighb=num_nearest_neighb,\n",
    "        scaling_ratio=novelty_scaling_ratio,\n",
    "    )\n",
    ")\n",
    "\n",
    "# define the QDPG emitter\n",
    "qdpg_emitter = QDPGEmitter(\n",
    "    config=qdpg_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    score_novelty=score_novelty,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and initialise the MAP Elites algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=qdpg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_bd,\n",
    "    maxval=max_bd,\n",
    "    random_key=random_key,\n",
    ")\n",
    "\n",
    "# compute initial repertoire\n",
    "repertoire, emitter_state, random_key = map_elites.init(\n",
    "    init_variables, centroids, random_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = int(num_iterations / log_period)\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"qdpg-logs.csv\",\n",
    "    header=[\"loop\", \"iteration\", \"qd_score\", \"max_fitness\", \"coverage\", \"time\"]\n",
    ")\n",
    "all_metrics = {}\n",
    "\n",
    "# main loop\n",
    "map_elites_scan_update = map_elites.scan_update\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    # main iterations\n",
    "    (repertoire, emitter_state, random_key,), metrics = jax.lax.scan(\n",
    "        map_elites_scan_update,\n",
    "        (repertoire, emitter_state, random_key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # log metrics\n",
    "    logged_metrics = {\"time\": timelapse, \"loop\": 1+i, \"iteration\": 1 + i*log_period}\n",
    "    for key, value in metrics.items():\n",
    "        # take last value\n",
    "        logged_metrics[key] = value[-1]\n",
    "\n",
    "        # take all values\n",
    "        if key in all_metrics.keys():\n",
    "            all_metrics[key] = jnp.concatenate([all_metrics[key], value])\n",
    "        else:\n",
    "            all_metrics[key] = value\n",
    "\n",
    "    csv_logger.log(logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# create the x-axis array\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * env_batch_size\n",
    "\n",
    "# create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(env_steps=env_steps, metrics=all_metrics, repertoire=repertoire, min_bd=min_bd, max_bd=max_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
