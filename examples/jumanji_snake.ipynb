{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Training a population on Jumanji-Snake with QDax\n",
    "\n",
    "This notebook shows how to use either MAP-Elites or a simple (non-QD) genetic algorithm to train a population of agents that play the game of Snake from [Jumanji](https://github.com/instadeepai/jumanji).\n",
    "\n",
    "This notebook can be used as an inspiration to interact with other environments from Jumanji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    print(\"QDax not found. Installing...\")\n",
    "    !pip install qdax[cuda12]\n",
    "    import qdax\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Tuple, Type\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import functools\n",
    "\n",
    "import jumanji\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from qdax.baselines.genetic_algorithm import GeneticAlgorithm\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.mapelites_repertoire import compute_euclidean_centroids\n",
    "\n",
    "\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "\n",
    "from qdax.tasks.jumanji_envs import jumanji_scoring_function\n",
    "\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.custom_types import ExtraScores, Fitness, RNGKey, Descriptor\n",
    "from qdax.utils.metrics import default_ga_metrics, default_qd_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "policy_hidden_layer_sizes = (128, 128)\n",
    "episode_length = 200\n",
    "population_size = 100\n",
    "batch_size = population_size\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "iso_sigma = 0.005\n",
    "line_sigma = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Instantiate the snake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Jumanji environment using the registry\n",
    "env = jumanji.make('Snake-v1')\n",
    "\n",
    "# Reset your (jit-able) environment\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "state, timestep = jax.jit(env.reset)(subkey)\n",
    "\n",
    "# Interact with the (jit-able) environment\n",
    "action = env.action_spec().generate_value()          # Action selection (dummy value here)\n",
    "state, timestep = jax.jit(env.step)(state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Define the type of policy that will be used to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions\n",
    "num_actions = env.action_spec().maximum + 1\n",
    "\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (num_actions,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jax.nn.softmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Utils to interact with the environment\n",
    "\n",
    "Define a way to process the observation and define a way to play a step in the environment, given the parameters of a policy_network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_processing(observation):\n",
    "    network_input = jnp.concatenate([jnp.ravel(observation.grid), jnp.array([observation.step_count]), observation.action_mask.ravel()])\n",
    "    return network_input\n",
    "\n",
    "\n",
    "def play_step_fn(\n",
    "    env_state,\n",
    "    timestep,\n",
    "    policy_params,\n",
    "    key,\n",
    "):\n",
    "    \"\"\"Play an environment step and return the updated state and the transition.\n",
    "    Everything is deterministic in this simple example.\n",
    "    \"\"\"\n",
    "\n",
    "    network_input = observation_processing(timestep.observation)\n",
    "\n",
    "    proba_action = policy_network.apply(policy_params, network_input)\n",
    "\n",
    "    action = jnp.argmax(proba_action)\n",
    "\n",
    "    state_desc = None\n",
    "    next_state, next_timestep = env.step(env_state, action)\n",
    "\n",
    "    # next_state_desc=next_state.info[\"state_descriptor\"]\n",
    "    next_state_desc = None\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=timestep.observation,\n",
    "        next_obs=next_timestep.observation,\n",
    "        rewards=next_timestep.reward,\n",
    "        dones=jnp.where(next_timestep.last(), jnp.array(1), jnp.array(0)),\n",
    "        actions=action,\n",
    "        truncations=jnp.array(0),\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state_desc,\n",
    "    )\n",
    "\n",
    "    return next_state, next_timestep, policy_params, key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Init a population of policies\n",
    "\n",
    "Also init init states and timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population of controllers\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, num=batch_size)\n",
    "\n",
    "# compute observation size from observation spec\n",
    "obs_spec = env.observation_spec()\n",
    "observation_size = int(np.prod(obs_spec.grid.shape) + np.prod(obs_spec.step_count.shape) + np.prod(obs_spec.action_mask.shape))\n",
    "\n",
    "fake_batch = jnp.zeros(shape=(batch_size, observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "\n",
    "# Create the initial environment states\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "\n",
    "init_states, init_timesteps = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Define a method to extract descriptor when relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "def descriptor_extraction(data: QDTransition, mask: jnp.ndarray, linear_projection: jnp.ndarray) -> Descriptor:\n",
    "    \"\"\"Compute feet contact time proportion.\n",
    "\n",
    "    This function suppose that state descriptor is the feet contact, as it\n",
    "    just computes the mean of the state descriptors given.\n",
    "    \"\"\"\n",
    "\n",
    "    # pre-process the observation\n",
    "    observation = jax.vmap(jax.vmap(observation_processing))(data.obs)\n",
    "\n",
    "    # get the mean\n",
    "    mean_observation = jnp.mean(observation, axis=-2)\n",
    "\n",
    "    # project those in [-1, 1]^2\n",
    "    descriptors = jnp.tanh(mean_observation @ linear_projection.T)\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "# create a random projection to a two dim space\n",
    "key, subkey = jax.random.split(key)\n",
    "linear_projection = jax.random.uniform(\n",
    "    subkey, (2, observation_size), minval=-1, maxval=1, dtype=jnp.float32\n",
    ")\n",
    "\n",
    "descriptor_extraction_fn = functools.partial(\n",
    "    descriptor_extraction,\n",
    "    linear_projection=linear_projection\n",
    ")\n",
    "\n",
    "# define the scoring function\n",
    "scoring_fn = functools.partial(\n",
    "    jumanji_scoring_function,\n",
    "    init_states=init_states,\n",
    "    init_timesteps=init_timesteps,\n",
    "    episode_length=episode_length,\n",
    "    play_step_fn=play_step_fn,\n",
    "    descriptor_extractor=descriptor_extraction_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Define the scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function(\n",
    "    genotypes: jnp.ndarray, key: RNGKey\n",
    ") -> Tuple[Fitness, ExtraScores, RNGKey]:\n",
    "    fitnesses, _, extra_scores = scoring_fn(genotypes, key)\n",
    "    return fitnesses.reshape(-1, 1), extra_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Define the emitter used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=None,\n",
    "    variation_fn=variation_fn,\n",
    "    variation_percentage=1.0,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Define the algorithm used and apply the initial step\n",
    "\n",
    "One can either use a simple genetic algorithm or use MAP-Elites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_map_elites = True\n",
    "\n",
    "if not use_map_elites:\n",
    "    algo_instance = GeneticAlgorithm(\n",
    "        scoring_function=scoring_function,\n",
    "        emitter=mixing_emitter,\n",
    "        metrics_function=default_ga_metrics,\n",
    "    )\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algo_instance.init(\n",
    "        init_variables, population_size, subkey\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Define a metrics function\n",
    "    metrics_function = functools.partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=0,\n",
    "    )\n",
    "\n",
    "    # Instantiate MAP-Elites\n",
    "    algo_instance = MAPElites(\n",
    "        scoring_function=scoring_fn,\n",
    "        emitter=mixing_emitter,\n",
    "        metrics_function=metrics_function,\n",
    "    )\n",
    "\n",
    "    # Compute the centroids\n",
    "    centroids = compute_euclidean_centroids(\n",
    "        grid_shape=(50, 50),\n",
    "        minval=-1,\n",
    "        maxval=1,\n",
    "    )\n",
    "\n",
    "    # Compute initial repertoire and emitter state\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algo_instance.init(init_variables, centroids, subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Run the optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the algorithm\n",
    "(repertoire, emitter_state, key,), metrics = jax.lax.scan(\n",
    "    algo_instance.scan_update,\n",
    "    (repertoire, emitter_state, key),\n",
    "    (),\n",
    "    length=num_iterations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"max_fitness\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire.fitnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire.descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the x-axis array\n",
    "env_steps = jnp.arange(num_iterations) * episode_length * batch_size\n",
    "\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "\n",
    "# create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(\n",
    "    env_steps=env_steps, metrics=metrics, repertoire=repertoire, min_descriptor=-1., max_descriptor=1.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Play snake with the best policy\n",
    "\n",
    "Retrieve one of the best policies from the repertoire and show how it does on the Snake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = jnp.argmax(repertoire.fitnesses)\n",
    "best_fitness = jnp.max(repertoire.fitnesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Best fitness in the repertoire: {best_fitness:.2f}\\n\",\n",
    "    f\"Index in the repertoire of this individual: {best_idx}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = jax.tree.map(\n",
    "    lambda x: x[best_idx],\n",
    "    repertoire.genotypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = jax.tree.map(\n",
    "    lambda x: x[0],\n",
    "    init_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_timestep = jax.tree.map(\n",
    "    lambda x: x[0],\n",
    "    init_timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = jax.tree.map(lambda x: x.copy(), init_state)\n",
    "timestep = jax.tree.map(lambda x: x.copy(), init_timestep)\n",
    "\n",
    "for _ in range(100):\n",
    "    # (Optional) Render the env state\n",
    "    env.render(state)\n",
    "\n",
    "    network_input = observation_processing(timestep.observation)\n",
    "\n",
    "    proba_action = policy_network.apply(my_params, network_input)\n",
    "\n",
    "    action = jnp.argmax(proba_action)\n",
    "\n",
    "\n",
    "    state, timestep = jax.jit(env.step)(state, action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('qdaxpy38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
