{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adaptive-intelligent-robotics/QDax/blob/main/examples/aurora.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with AURORA in JAX\n",
    "\n",
    "This notebook shows how to use QDax to find diverse and performing controllers in MDPs with [AURORA](https://arxiv.org/pdf/1905.11874.pdf).\n",
    "It can be run locally or on Google Colab. We recommend to use a GPU. This notebook will show:\n",
    "\n",
    "- how to define the problem\n",
    "- how to create an emitter\n",
    "- how to create an AURORA instance\n",
    "- which functions must be defined before training\n",
    "- how to launch a certain number of training steps\n",
    "- how to visualise the optimization process\n",
    "- how to save/load a repertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    print(\"QDax not found. Installing...\")\n",
    "    !pip install qdax[cuda12]\n",
    "    import qdax\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl | tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import functools\n",
    "from typing import Dict, Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.aurora import AURORA\n",
    "from qdax.core.containers.unstructured_repertoire import UnstructuredRepertoire\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import (\n",
    "    create_default_brax_task_components,\n",
    "    get_aurora_scoring_fn,\n",
    ")\n",
    "from qdax.environments.descriptor_extractors import (\n",
    "    AuroraExtraInfoNormalization,\n",
    "    get_aurora_encoding,\n",
    ")\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "\n",
    "from qdax.custom_types import Observation\n",
    "from qdax.utils import train_seq2seq\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "    colab_tpu.setup_tpu()\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "batch_size = 100 #@param {type:\"number\"}\n",
    "env_name = 'walker2d_uni'#@param['ant_uni', 'hopper_uni', 'walker2d_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "episode_length = 250 #@param {type:\"integer\"}\n",
    "max_iterations = 50 #@param {type:\"integer\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "iso_sigma = 0.005 #@param {type:\"number\"}\n",
    "line_sigma = 0.05 #@param {type:\"number\"}\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_descriptor = 0. #@param {type:\"number\"}\n",
    "max_descriptor = 1.0 #@param {type:\"number\"}\n",
    "\n",
    "lstm_batch_size = 128 #@param {type:\"integer\"}\n",
    "\n",
    "observation_option = \"no_sd\" #@param['no_sd', 'only_sd', 'full']\n",
    "hidden_size = 5 #@param {type:\"integer\"}\n",
    "l_value_init = 0.2 #@param {type:\"number\"}\n",
    "\n",
    "traj_sampling_freq = 10 #@param {type:\"integer\"}\n",
    "max_observation_size = 25 #@param {type:\"integer\"}\n",
    "prior_descriptor_dim = 2 #@param {type:\"integer\"}\n",
    "\n",
    "log_freq = 5 #@param {type:\"integer\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env\n",
    "\n",
    "Define the environment in which the policies will be trained. In this notebook, we focus on controllers learning to move a robot in a physical simulation. We also define the shared policy, that every individual in the population will use. Once the policy is defined, all individuals are defined by their parameters, that corresponds to their genotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "\n",
    "# Init a random key\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, num=batch_size)\n",
    "fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "\n",
    "\n",
    "# Create the initial environment states\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jnp.repeat(jnp.expand_dims(subkey, axis=0), repeats=batch_size, axis=0)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "init_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env\n",
    "\n",
    "Now that the environment and policy has been defined, it is necessary to define a function that describes how the policy must be used to interact with the environment and to store transition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "    env_state,\n",
    "    policy_params,\n",
    "    key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scoring function and the way metrics are computed\n",
    "\n",
    "The scoring function is used in the evaluation step to determine the fitness and descriptor of each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "key, subkey = jax.random.split(key)\n",
    "env, policy_network, scoring_fn = create_default_brax_task_components(\n",
    "    env_name=env_name,\n",
    "    key=subkey,\n",
    ")\n",
    "\n",
    "def observation_extractor_fn(\n",
    "    data: QDTransition,\n",
    ") -> Observation:\n",
    "    \"\"\"Extract observation from the state.\"\"\"\n",
    "    state_obs = data.obs[:, ::traj_sampling_freq, :max_observation_size]\n",
    "\n",
    "    # add the x/y position - (batch_size, traj_length, 2)\n",
    "    state_desc = data.state_desc[:, ::traj_sampling_freq]\n",
    "\n",
    "    if observation_option == \"full\":\n",
    "        observations = jnp.concatenate([state_desc, state_obs], axis=-1)\n",
    "    elif observation_option == \"no_sd\":\n",
    "        observations = state_obs\n",
    "    elif observation_option == \"only_sd\":\n",
    "        observations = state_desc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown observation option.\")\n",
    "\n",
    "    return observations\n",
    "\n",
    "# Prepare the scoring function\n",
    "aurora_scoring_fn = get_aurora_scoring_fn(\n",
    "    scoring_fn=scoring_fn,\n",
    "    observation_extractor_fn=observation_extractor_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "def metrics_fn(repertoire: UnstructuredRepertoire) -> Dict:\n",
    "\n",
    "    # Get metrics\n",
    "    grid_empty = repertoire.fitnesses == -jnp.inf\n",
    "    qd_score = jnp.sum(repertoire.fitnesses, where=~grid_empty)\n",
    "    # Add offset for positive qd_score\n",
    "    qd_score += reward_offset * episode_length * jnp.sum(1.0 - grid_empty)\n",
    "    coverage = 100 * jnp.mean(1.0 - grid_empty)\n",
    "    max_fitness = jnp.max(repertoire.fitnesses)\n",
    "\n",
    "    return {\"qd_score\": qd_score, \"max_fitness\": max_fitness, \"coverage\": coverage}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the emitter\n",
    "\n",
    "The emitter is used to evolve the population at each mutation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=lambda x, y: (x, y),\n",
    "    variation_fn=variation_fn,\n",
    "    variation_percentage=1.0,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and initialise the MAP Elites algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurora_dims = hidden_size\n",
    "centroids = jnp.zeros(shape=(num_centroids, aurora_dims))\n",
    "\n",
    "@jax.jit\n",
    "def update_scan_fn(carry: Any, _: Any) -> Any:\n",
    "    \"\"\"Scan the update function.\"\"\"\n",
    "    repertoire, key, aurora_extra_info = carry\n",
    "\n",
    "    # update\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, _, metrics = aurora.update(\n",
    "        repertoire,\n",
    "        None,\n",
    "        subkey,\n",
    "        aurora_extra_info=aurora_extra_info,\n",
    "    )\n",
    "\n",
    "    return (repertoire, key, aurora_extra_info), metrics\n",
    "\n",
    "# Init algorithm\n",
    "# AutoEncoder Params and INIT\n",
    "obs_dim = jnp.minimum(env.observation_size, max_observation_size)\n",
    "if observation_option == \"full\":\n",
    "    observations_dims = (\n",
    "        episode_length // traj_sampling_freq,\n",
    "        obs_dim + prior_descriptor_dim,\n",
    "    )\n",
    "elif observation_option == \"no_sd\":\n",
    "    observations_dims = (\n",
    "        episode_length // traj_sampling_freq,\n",
    "        obs_dim,\n",
    "    )\n",
    "elif observation_option == \"only_sd\":\n",
    "    observations_dims = (episode_length // traj_sampling_freq, prior_descriptor_dim)\n",
    "else:\n",
    "    ValueError(\"The chosen option is not correct.\")\n",
    "\n",
    "# Define the seq2seq model\n",
    "model = train_seq2seq.get_model(\n",
    "    observations_dims[-1], True, hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "# Init the model params\n",
    "key, subkey = jax.random.split(key)\n",
    "model_params = train_seq2seq.get_initial_params(\n",
    "    model, subkey, (1, *observations_dims)\n",
    ")\n",
    "\n",
    "print(jax.tree_map(lambda x: x.shape, model_params))\n",
    "\n",
    "# Define the encoder function\n",
    "encoder_fn = jax.jit(\n",
    "    functools.partial(\n",
    "        get_aurora_encoding,\n",
    "        model=model,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the training function\n",
    "train_fn = functools.partial(\n",
    "    train_seq2seq.lstm_ae_train,\n",
    "    model=model,\n",
    "    batch_size=lstm_batch_size,\n",
    ")\n",
    "\n",
    "# Instantiate AURORA\n",
    "aurora = AURORA(\n",
    "    scoring_function=aurora_scoring_fn,\n",
    "    emitter=mixing_emitter,\n",
    "    metrics_function=metrics_fn,\n",
    "    encoder_function=encoder_fn,\n",
    "    training_function=train_fn,\n",
    ")\n",
    "\n",
    "# define arbitrary observation's mean/std\n",
    "mean_observations = jnp.zeros(observations_dims[-1])\n",
    "std_observations = jnp.ones(observations_dims[-1])\n",
    "\n",
    "# init all the information needed by AURORA to compute encodings\n",
    "aurora_extra_info = AuroraExtraInfoNormalization.create(\n",
    "    model_params,\n",
    "    mean_observations,\n",
    "    std_observations,\n",
    ")\n",
    "\n",
    "# init step of the aurora algorithm\n",
    "key, subkey = jax.random.split(key)\n",
    "repertoire, emitter_state, init_metrics, aurora_extra_info = aurora.init(\n",
    "    init_variables,\n",
    "    aurora_extra_info,\n",
    "    jnp.asarray(l_value_init),\n",
    "    max_observation_size,\n",
    "    subkey,\n",
    ")\n",
    "\n",
    "# initializing means and stds and AURORA\n",
    "key, subkey = jax.random.split(key)\n",
    "repertoire, aurora_extra_info = aurora.train(\n",
    "    repertoire, model_params, iteration=0, key=subkey\n",
    ")\n",
    "\n",
    "# design aurora's schedule\n",
    "default_update_base = 10\n",
    "update_base = int(jnp.ceil(default_update_base / log_freq))\n",
    "schedules = jnp.cumsum(jnp.arange(update_base, 1000, update_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch AURORA iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step_estimation = 0\n",
    "num_iterations = 0\n",
    "\n",
    "# Main loop\n",
    "n_target = 1024\n",
    "\n",
    "previous_error = jnp.sum(repertoire.fitnesses != -jnp.inf) - n_target\n",
    "\n",
    "iteration = 0\n",
    "while iteration < max_iterations:\n",
    "\n",
    "    (\n",
    "        (repertoire, key, aurora_extra_info),\n",
    "        metrics,\n",
    "    ) = jax.lax.scan(\n",
    "        update_scan_fn,\n",
    "        (repertoire, key, aurora_extra_info),\n",
    "        (),\n",
    "        length=log_freq,\n",
    "    )\n",
    "\n",
    "    num_iterations = iteration * log_freq\n",
    "\n",
    "    # update nb steps estimation\n",
    "    current_step_estimation += batch_size * episode_length * log_freq\n",
    "\n",
    "    # autoencoder steps and CVC\n",
    "    if (iteration + 1) in schedules:\n",
    "        # train the autoencoder\n",
    "        key, subkey = jax.random.split(key)\n",
    "        repertoire, aurora_extra_info = aurora.train(\n",
    "            repertoire, model_params, iteration, subkey\n",
    "        )\n",
    "\n",
    "    elif iteration % 2 == 0:\n",
    "        repertoire, previous_error = aurora.container_size_control(\n",
    "            repertoire,\n",
    "            target_size=n_target,\n",
    "            previous_error=previous_error,\n",
    "        )\n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in metrics.items():\n",
    "    print(k, \" - \", v[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
