{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adaptive-intelligent-robotics/QDax/blob/main/examples/pgame.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with DCRL-ME in JAX\n",
    "\n",
    "This notebook shows how to use QDax to find diverse and performing controllers in MDPs with [Descriptor-Conditioned Reinforcement Learning MAP-Elites (DCRL-ME)](https://arxiv.org/abs/2401.08632).\n",
    "This algorithm extends and improves upon [Descriptor-Conditioned Gradients MAP-Elites (DCG-ME)](https://dl.acm.org/doi/abs/10.1145/3583131.3590503)\n",
    "It can be run locally or on Google Colab. We recommend to use a GPU. This notebook will show:\n",
    "\n",
    "- how to define the problem\n",
    "- how to create the DCRL emitter\n",
    "- how to create a Map-elites instance\n",
    "- which functions must be defined before training\n",
    "- how to launch a certain number of training steps\n",
    "- how to visualize the results of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    import qdax\n",
    "except:\n",
    "    print(\"QDax not found. Installing...\")\n",
    "    !pip install qdax[cuda12]\n",
    "    import qdax\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl | tail -n 1\n",
    "# %matplotlib widget\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import functools\n",
    "import time\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax import environments\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids\n",
    "from qdax.core.emitters.dcrl_me_emitter import DCRLMEConfig, DCRLMEEmitter\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.neuroevolution.buffers.buffer import DCRLTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP, MLPDC\n",
    "from qdax.custom_types import EnvState, Params, RNGKey\n",
    "from qdax.environments import descriptor_extractor\n",
    "from qdax.environments.wrappers import OffsetRewardWrapper, ClipRewardWrapper\n",
    "from qdax.tasks.brax_envs import scoring_function_brax_envs\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "\n",
    "env_name = \"ant_omni\" #@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "episode_length = 250 #@param {type:\"integer\"}\n",
    "min_descriptor = -30.0 #@param {type:\"number\"}\n",
    "max_descriptor = 30.0 #@param {type:\"number\"}\n",
    "\n",
    "num_iterations = 1000 #@param {type:\"integer\"}\n",
    "batch_size = 256 #@param {type:\"integer\"}\n",
    "\n",
    "# Archive\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (128, 128) #@param {type:\"raw\"}\n",
    "\n",
    "# DCRL-ME\n",
    "ga_batch_size = 128 #@param {type:\"integer\"}\n",
    "dcrl_batch_size = 64 #@param {type:\"integer\"}\n",
    "ai_batch_size = 64 #@param {type:\"integer\"}\n",
    "lengthscale = 0.1 #@param {type:\"number\"}\n",
    "\n",
    "# GA emitter\n",
    "iso_sigma = 0.005 #@param {type:\"number\"}\n",
    "line_sigma = 0.05 #@param {type:\"number\"}\n",
    "\n",
    "# DCRL emitter\n",
    "critic_hidden_layer_size = (256, 256) #@param {type:\"raw\"}\n",
    "num_critic_training_steps = 3000 #@param {type:\"integer\"}\n",
    "num_pg_training_steps = 150 #@param {type:\"integer\"}\n",
    "replay_buffer_size = 1_000_000 #@param {type:\"integer\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "critic_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 #@param {type:\"number\"}\n",
    "policy_learning_rate = 5e-3 #@param {type:\"number\"}\n",
    "noise_clip = 0.5 #@param {type:\"number\"}\n",
    "policy_noise = 0.2 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "policy_delay = 2 #@param {type:\"number\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init environment, policy, population params, init states of the env\n",
    "\n",
    "Define the environment in which the policies will be trained. In this notebook, we focus on controllers learning to move a robot in a physical simulation. We also define the shared policy, that every individual in the population will use. Once the policy is defined, all individuals are defined by their parameters, that corresponds to their genotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a random key\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "# Init environment\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "env = OffsetRewardWrapper(\n",
    "    env, offset=environments.reward_offset[env_name]\n",
    ")  # apply reward offset as DCRL needs positive rewards\n",
    "env = ClipRewardWrapper(\n",
    "    env, clip_min=0.,\n",
    ")  # apply reward clip as DCRL needs positive rewards\n",
    "reset_fn = jax.jit(env.reset)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "actor_dc_network = MLPDC(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, num=batch_size)\n",
    "fake_batch_obs = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the way the policy interacts with the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "    env_state: EnvState, policy_params: Params, key: RNGKey\n",
    ") -> Tuple[EnvState, Params, RNGKey, DCRLTransition]:\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = DCRLTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        desc=jnp.zeros(\n",
    "            env.descriptor_length,\n",
    "        )\n",
    "        * jnp.nan,\n",
    "        desc_prime=jnp.zeros(\n",
    "            env.descriptor_length,\n",
    "        )\n",
    "        * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, key, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scoring function and the way metrics are computed\n",
    "\n",
    "The scoring function is used in the evaluation step to determine the fitness and behavior descriptor of each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "descriptor_extraction_fn = descriptor_extractor[env_name]\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function_brax_envs,\n",
    "    episode_length=episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    descriptor_extractor=descriptor_extraction_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the emitter: DCRL Emitter\n",
    "\n",
    "The emitter is used to evolve the population at each mutation step. In this example, the emitter is the Descriptor-Conditioned RL emitter, the one used in DCRL-ME. It trains a critic with the transitions experienced in the environment and uses the critic to apply Descriptor-Conditioned gradients updates to the policies evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcrl_emitter_config = DCRLMEConfig(\n",
    "    ga_batch_size=ga_batch_size,\n",
    "    dcrl_batch_size=dcrl_batch_size,\n",
    "    ai_batch_size=ai_batch_size,\n",
    "    lengthscale=lengthscale,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps,\n",
    "    batch_size=batch_size,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    actor_learning_rate=actor_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    policy_delay=policy_delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "\n",
    "dcrl_emitter = DCRLMEEmitter(\n",
    "    config=dcrl_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    actor_network=actor_dc_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and initialise the MAP Elites algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=dcrl_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "key, subkey = jax.random.split(key)\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=env.descriptor_length,\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_descriptor,\n",
    "    maxval=max_descriptor,\n",
    "    key=subkey,\n",
    ")\n",
    "\n",
    "# compute initial repertoire\n",
    "key, subkey = jax.random.split(key)\n",
    "repertoire, emitter_state, init_metrics = map_elites.init(\n",
    "    init_params, centroids, subkey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = num_iterations // log_period\n",
    "\n",
    "metrics = dict.fromkeys([\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"], jnp.array([]))\n",
    "csv_logger = CSVLogger(\n",
    "    \"dcrlme-logs.csv\",\n",
    "    header=list(metrics.keys())\n",
    ")\n",
    "\n",
    "# Main loop\n",
    "map_elites_scan_update = map_elites.scan_update\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    (\n",
    "        repertoire,\n",
    "        emitter_state,\n",
    "        key,\n",
    "    ), current_metrics = jax.lax.scan(\n",
    "        map_elites_scan_update,\n",
    "        (repertoire, emitter_state, key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # Metrics\n",
    "    current_metrics[\"iteration\"] = jnp.arange(1+log_period*i, 1+log_period*(i+1), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    metrics = jax.tree.map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics, current_metrics)\n",
    "\n",
    "    # Log\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "\n",
    "# Create the x-axis array\n",
    "env_steps = metrics[\"iteration\"]\n",
    "\n",
    "%matplotlib inline\n",
    "# Create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(env_steps=env_steps, metrics=metrics, repertoire=repertoire, min_descriptor=min_descriptor, max_descriptor=max_descriptor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ae46cf6a59eb5e192bc4f27fbb5c33d8a30eb9acb43edbb510eeaf7c819ab64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
